### 基础环境部署

#### JDK部署
  
下载java1.8jdk

```
yum install java-1.8.0-openjdk-devel.x86_64
```
配置环境变量
```
export JAVA_HOME=/opt/java/jdk1.8.0_91
export PATH=$JAVA_HOME/bin:$PATH
```

> 若是通过命令安装，则不必配置系统环境变量

#### Maven部署

下载maven
```
wget https://dlcdn.apache.org/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz --no-check-certificate
```
配置环境变量
```
# maven环境变量
export MAVEN_HOME=/opt/maven/apache-maven-3.8.6
export PATH=$MAVEN_HOME/bin:$PATH
```

检查mvn环境，若显示以下类似内容则表明安装成功

```
[root@hadoop apache-maven-3.8.6]# mvn -v
Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63)
Maven home: /opt/maven/apache-maven-3.8.6
Java version: 1.8.0_262, vendor: Oracle Corporation, runtime: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-1.el7.x86_64/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "3.10.0-1160.el7.x86_64", arch: "amd64", family: "unix"
```

#### Hadoop

下载地址:https://archive.cloudera.com/p/cdh5/cdh/5/   

版本：hadoop-2.6.0-cdh5.15.1

配置hadoop环境变量

```
export HADOOP_HOME=/opt/hadoop-2.6.0-cdh5.15.1
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

设置SSH免密码登录

```
[root@hadoop hadoop-2.6.0-cdh5.15.1]# ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:YNKR8kT8efwgClYepGw1EqK634syTMRC+vfFJ6Lfuhg root@hadoop
The key's randomart image is:
+---[RSA 2048]----+
|  . o=*.         |
| o o.=*o         |
|=   =*+o o       |
|+o .ooo.+ +      |
|+. . . oSo o     |
| o. . o + . .    |
|+  .Eo o o       |
|oo o.o..         |
| oo +o+o.        |
+----[SHA256]-----+
[root@hadoop hadoop-2.6.0-cdh5.15.1]# ssh-copy-id hadoop
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
The authenticity of host 'hadoop (192.168.42.189)' can't be established.
ECDSA key fingerprint is SHA256:m8tPjgDpwMfdir/vHoYNYCF6t1wZ6293TrdsA40IO9c.
ECDSA key fingerprint is MD5:66:59:33:51:ae:05:75:2d:a7:87:1c:76:14:a1:5f:f2.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
root@hadoop's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'hadoop'"
and check to make sure that only the key(s) you wanted were added.

[root@hadoop hadoop-2.6.0-cdh5.15.1]# ssh hadoop
Last login: Fri Jul  1 23:56:14 2022 from 192.168.42.1
[root@hadoop ~]# exit
logout
Connection to hadoop closed.
[root@hadoop hadoop-2.6.0-cdh5.15.1]# 
```

修改hadoop配置

- etc/hadoop/hadoop-env.sh

```
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-1.el7.x86_64/jre
```

- etc/hadoop/core-site.xml:

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

- etc/hadoop/hdfs-site.xml:

```
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/liutao/tmp/dfs/name</value>
    </property>
        <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/liutao/tmp/dfs/data</value>
    </property>
    <!-- 咱们当前就一个节点，所以副本系数配置为1 -->
     <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
     <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>
```

- etc/hadoop/yarn-site.xml:

```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

- etc/hadoop/mapred-site.xml:

```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

- 格式化HDFS文件系统

在启动HDFS之前，一定要先对HDFS格式化
> 切记：格式化只需做一次，因为一旦格式化，那么HDFS文件系统上的数据就不存在了


```
bin/hdfs namenode -format
```

若输出以下这句，则表明文件系统格式化成功

```
22/07/02 00:35:33 INFO common.Storage: Storage directory /home/liutao/tmp/dfs/name has been successfully formatted.
```

- 启动方式

1）逐个启动/停止

```
hadoop-daemon.sh start/stop namenode
hadoop-daemon.sh start/stop datanode
```
> 启动的时候先启动namenode，停止的时候则相反

然后使用jps查看进程启动情况
> 提示：jps命令是由java的jdk提供，若执行该指令时报错，则需要检查java的jdk安装

```
[root@hadoop hadoop]# jps
51768 NameNode
51850 DataNode
52395 Jps
```

> 若发现有进程缺失，那么就需要查看缺失的进行的相关日志（log）

2)使用 `./start-dfs.sh` 一键式启动

```
[root@hadoop sbin]# ./start-dfs.sh 

22/07/02 01:01:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [hadoop]
hadoop: starting namenode, logging to /opt/hadoop-2.6.0-cdh5.15.1/logs/hadoop-root-namenode-hadoop.out
The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is SHA256:m8tPjgDpwMfdir/vHoYNYCF6t1wZ6293TrdsA40IO9c.
ECDSA key fingerprint is MD5:66:59:33:51:ae:05:75:2d:a7:87:1c:76:14:a1:5f:f2.
Are you sure you want to continue connecting (yes/no)? yes
localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
localhost: starting datanode, logging to /opt/hadoop-2.6.0-cdh5.15.1/logs/hadoop-root-datanode-hadoop.out
Starting secondary namenodes [0.0.0.0]
The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
ECDSA key fingerprint is SHA256:m8tPjgDpwMfdir/vHoYNYCF6t1wZ6293TrdsA40IO9c.
ECDSA key fingerprint is MD5:66:59:33:51:ae:05:75:2d:a7:87:1c:76:14:a1:5f:f2.
Are you sure you want to continue connecting (yes/no)? yes
0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.6.0-cdh5.15.1/logs/hadoop-root-secondarynamenode-hadoop.out
22/07/02 01:02:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@hadoop sbin]# jps
52964 SecondaryNameNode
53078 Jps
52669 NameNode
52799 DataNode
```

通过日志，我们可以看到该指令分别启动了`namenodes`,`secondary namenodes`等服务


停止服务
```
 ./stop-dfs.sh 
```

- 启动yarn服务

```
[root@hadoop sbin]# ./start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop-2.6.0-cdh5.15.1/logs/yarn-root-resourcemanager-hadoop.out
localhost: starting nodemanager, logging to /opt/hadoop-2.6.0-cdh5.15.1/logs/yarn-root-nodemanager-hadoop.out
[root@hadoop sbin]# jps
54860 ResourceManager
54956 NodeManager
55278 Jps
[root@hadoop sbin]# 
```

- 查看yarn集群服务

``` 
http://127.0.0.1:8088/cluster
```

- 检查yarn服务是否可以正常运行

```
[root@hadoop mapreduce]# hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar pi 3 4
Number of Maps  = 3
Samples per Map = 4
22/07/02 01:24:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Starting Job
22/07/02 01:24:43 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
22/07/02 01:24:43 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
22/07/02 01:24:43 INFO input.FileInputFormat: Total input paths to process : 3
22/07/02 01:24:43 INFO mapreduce.JobSubmitter: number of splits:3
22/07/02 01:24:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local228312636_0001
22/07/02 01:24:44 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
22/07/02 01:24:44 INFO mapreduce.Job: Running job: job_local228312636_0001
22/07/02 01:24:44 INFO mapred.LocalJobRunner: OutputCommitter set in config null
22/07/02 01:24:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/02 01:24:44 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/07/02 01:24:44 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Waiting for map tasks
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Starting task: attempt_local228312636_0001_m_000000_0
22/07/02 01:24:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/02 01:24:44 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/07/02 01:24:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
22/07/02 01:24:44 INFO mapred.MapTask: Processing split: hdfs://hadoop:8020/user/root/QuasiMonteCarlo_1656750280758_1971228792/in/part0:0+118
22/07/02 01:24:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
22/07/02 01:24:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
22/07/02 01:24:44 INFO mapred.MapTask: soft limit at 83886080
22/07/02 01:24:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
22/07/02 01:24:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
22/07/02 01:24:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
22/07/02 01:24:44 INFO mapred.LocalJobRunner: 
22/07/02 01:24:44 INFO mapred.MapTask: Starting flush of map output
22/07/02 01:24:44 INFO mapred.MapTask: Spilling map output
22/07/02 01:24:44 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
22/07/02 01:24:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
22/07/02 01:24:44 INFO mapred.MapTask: Finished spill 0
22/07/02 01:24:44 INFO mapred.Task: Task:attempt_local228312636_0001_m_000000_0 is done. And is in the process of committing
22/07/02 01:24:44 INFO mapred.LocalJobRunner: map
22/07/02 01:24:44 INFO mapred.Task: Task 'attempt_local228312636_0001_m_000000_0' done.
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local228312636_0001_m_000000_0
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Starting task: attempt_local228312636_0001_m_000001_0
22/07/02 01:24:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/02 01:24:44 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/07/02 01:24:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
22/07/02 01:24:44 INFO mapred.MapTask: Processing split: hdfs://hadoop:8020/user/root/QuasiMonteCarlo_1656750280758_1971228792/in/part1:0+118
22/07/02 01:24:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
22/07/02 01:24:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
22/07/02 01:24:44 INFO mapred.MapTask: soft limit at 83886080
22/07/02 01:24:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
22/07/02 01:24:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
22/07/02 01:24:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
22/07/02 01:24:44 INFO mapred.LocalJobRunner: 
22/07/02 01:24:44 INFO mapred.MapTask: Starting flush of map output
22/07/02 01:24:44 INFO mapred.MapTask: Spilling map output
22/07/02 01:24:44 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
22/07/02 01:24:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
22/07/02 01:24:44 INFO mapred.MapTask: Finished spill 0
22/07/02 01:24:44 INFO mapred.Task: Task:attempt_local228312636_0001_m_000001_0 is done. And is in the process of committing
22/07/02 01:24:44 INFO mapred.LocalJobRunner: map
22/07/02 01:24:44 INFO mapred.Task: Task 'attempt_local228312636_0001_m_000001_0' done.
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local228312636_0001_m_000001_0
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Starting task: attempt_local228312636_0001_m_000002_0
22/07/02 01:24:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/02 01:24:44 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/07/02 01:24:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
22/07/02 01:24:44 INFO mapred.MapTask: Processing split: hdfs://hadoop:8020/user/root/QuasiMonteCarlo_1656750280758_1971228792/in/part2:0+118
22/07/02 01:24:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
22/07/02 01:24:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
22/07/02 01:24:44 INFO mapred.MapTask: soft limit at 83886080
22/07/02 01:24:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
22/07/02 01:24:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
22/07/02 01:24:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
22/07/02 01:24:44 INFO mapred.LocalJobRunner: 
22/07/02 01:24:44 INFO mapred.MapTask: Starting flush of map output
22/07/02 01:24:44 INFO mapred.MapTask: Spilling map output
22/07/02 01:24:44 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
22/07/02 01:24:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
22/07/02 01:24:44 INFO mapred.MapTask: Finished spill 0
22/07/02 01:24:44 INFO mapred.Task: Task:attempt_local228312636_0001_m_000002_0 is done. And is in the process of committing
22/07/02 01:24:44 INFO mapred.LocalJobRunner: map
22/07/02 01:24:44 INFO mapred.Task: Task 'attempt_local228312636_0001_m_000002_0' done.
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local228312636_0001_m_000002_0
22/07/02 01:24:44 INFO mapred.LocalJobRunner: map task executor complete.
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Waiting for reduce tasks
22/07/02 01:24:44 INFO mapred.LocalJobRunner: Starting task: attempt_local228312636_0001_r_000000_0
22/07/02 01:24:44 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/02 01:24:44 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/07/02 01:24:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
22/07/02 01:24:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@42e54562
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
22/07/02 01:24:44 INFO reduce.EventFetcher: attempt_local228312636_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
22/07/02 01:24:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local228312636_0001_m_000001_0 decomp: 24 len: 28 to MEMORY
22/07/02 01:24:44 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local228312636_0001_m_000001_0
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->24
22/07/02 01:24:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local228312636_0001_m_000002_0 decomp: 24 len: 28 to MEMORY
22/07/02 01:24:44 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local228312636_0001_m_000002_0
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 2, commitMemory -> 24, usedMemory ->48
22/07/02 01:24:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local228312636_0001_m_000000_0 decomp: 24 len: 28 to MEMORY
22/07/02 01:24:44 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local228312636_0001_m_000000_0
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 3, commitMemory -> 48, usedMemory ->72
22/07/02 01:24:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
22/07/02 01:24:44 INFO mapred.LocalJobRunner: 3 / 3 copied.
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs
22/07/02 01:24:44 INFO mapred.Merger: Merging 3 sorted segments
22/07/02 01:24:44 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 63 bytes
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: Merged 3 segments, 72 bytes to disk to satisfy reduce memory limit
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: Merging 1 files, 72 bytes from disk
22/07/02 01:24:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
22/07/02 01:24:44 INFO mapred.Merger: Merging 1 sorted segments
22/07/02 01:24:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 65 bytes
22/07/02 01:24:44 INFO mapred.LocalJobRunner: 3 / 3 copied.
22/07/02 01:24:44 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
22/07/02 01:24:44 INFO mapred.Task: Task:attempt_local228312636_0001_r_000000_0 is done. And is in the process of committing
22/07/02 01:24:44 INFO mapred.LocalJobRunner: 3 / 3 copied.
22/07/02 01:24:44 INFO mapred.Task: Task attempt_local228312636_0001_r_000000_0 is allowed to commit now
22/07/02 01:24:45 INFO output.FileOutputCommitter: Saved output of task 'attempt_local228312636_0001_r_000000_0' to hdfs://hadoop:8020/user/root/QuasiMonteCarlo_1656750280758_1971228792/out/_temporary/0/task_local228312636_0001_r_000000
22/07/02 01:24:45 INFO mapred.LocalJobRunner: reduce > reduce
22/07/02 01:24:45 INFO mapred.Task: Task 'attempt_local228312636_0001_r_000000_0' done.
22/07/02 01:24:45 INFO mapred.LocalJobRunner: Finishing task: attempt_local228312636_0001_r_000000_0
22/07/02 01:24:45 INFO mapred.LocalJobRunner: reduce task executor complete.
22/07/02 01:24:45 INFO mapreduce.Job: Job job_local228312636_0001 running in uber mode : false
22/07/02 01:24:45 INFO mapreduce.Job:  map 100% reduce 100%
22/07/02 01:24:45 INFO mapreduce.Job: Job job_local228312636_0001 completed successfully
22/07/02 01:24:45 INFO mapreduce.Job: Counters: 35
        File System Counters
                FILE: Number of bytes read=1110060
                FILE: Number of bytes written=2447548
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1062
                HDFS: Number of bytes written=1631
                HDFS: Number of read operations=46
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=23
        Map-Reduce Framework
                Map input records=3
                Map output records=6
                Map output bytes=54
                Map output materialized bytes=84
                Input split bytes=429
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=84
                Reduce input records=6
                Reduce output records=0
                Spilled Records=12
                Shuffled Maps =3
                Failed Shuffles=0
                Merged Map outputs=3
                GC time elapsed (ms)=15
                Total committed heap usage (bytes)=1228931072
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=354
        File Output Format Counters 
                Bytes Written=97
Job Finished in 1.579 seconds
Estimated value of Pi is 3.66666666666666666667
```

#### Hive 部署

下载与hadoop一致的hive版本，然后配置环境变量

```
# hive环境变量
export HIVE_HOME=/opt/hive-1.1.0-cdh5.15.1
export PATH=$HIVE_HOME/bin:$PATH
```

使用MySQL存储Hive的元数据，因此我们需要安装MySQL

一旦MySQL安装完成后，需要在$HIVE_HOME/lib库中添加MySQL驱动

> 注意：需要与MySQL的版本匹配。比如我这里使用的是MySQL5.7，因此我们使用的驱动版本为：mysql-connector-java-5.1.27-bin.jar

修改`$HIVE_HOME/conf/hive-site.xml`配置文件

```
<configuration>
<!-- 存储元数据mysql相关配置 -->
<property>
	<name>javax.jdo.option.ConnectionURL</name>
	<value>jdbc:mysql://hadoop:3306/hive3?createDatabaseIfNotExist=true&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
</property>

<property>
	<name>javax.jdo.option.ConnectionDriverName</name>
	<value>com.mysql.jdbc.Driver</value>
</property>

<property>
	<name>javax.jdo.option.ConnectionUserName</name>
	<value>root</value>
</property>

<property>
	<name>javax.jdo.option.ConnectionPassword</name>
	<value>123456</value>
</property>
</configuration>

```

启动Hive

```
./bin/hive
```

若在启动过程中，发现以下问题，则检查数据库访问权限配置

```
hive> show tables;
FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
```

建表

```
hive> create table pk(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
```

在`/opt/hive-1.1.0-cdh5.15.1/data`生成测试数据 pk.txt

```
1 张三
2 李四
3 王五
```


加载数据

```
hive> load data local inpath '/opt/hive-1.1.0-cdh5.15.1/data/pk.txt' overwrite into table pk;
```
#### Spark 安装


#### Spark运行模式
  
  1）local: 运用于本地开发
  
  2）standalone: 部署多个spark节点（生产环境中用的不多）
  
  3）yarn: 将spark作业提交到hadoop(Yarn)集群中运行，spark仅仅只是一个客户端而已。
  
  4）K8s: 2.3版本后才正式稍微稳定，是未来是一个比较好的方式



#### 在Yarn服务上启动Spark

- 在spark客户端配置`HADOOP_CONF_DIR`环境变量

Ensure that HADOOP_CONF_DIR or YARN_CONF_DIR points to the directory which contains the (client side) configuration files for the Hadoop cluster. These configs are used to write to HDFS and connect to the YARN ResourceManager. The configuration contained in this directory will be distributed to the YARN cluster so that all containers used by the application use the same configuration. If the configuration references Java system properties or environment variables not managed by YARN, they should also be set in the Spark application’s configuration (driver, executors, and the AM when running in client mode).

```
export HADOOP_CONF_DIR=/opt/hadoop-2.6.0-cdh5.15.1/etc/hadoop
```

- 提交任务

```
spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/spark-2.4.3-bin-2.6.0-cdh5.15.1/examples/jars/spark-examples_2.11-2.4.3.jar  
```

> 这里我们使用的是spark模块中example中的jar包